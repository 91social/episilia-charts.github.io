# Default values for episilia-spike.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

imageTag: &release "3.0.0"
replicaCount: 1

image:
  repository: episilia/spike
  pullPolicy: IfNotPresent
  tag: *release

imagePullSecrets:
  - name: regcred 

nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: true
  annotations: {}
  name: ""

podAnnotations: {}

podSecurityContext: {}

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP 

ingress:
  enabled: false
  annotations: {}
  hosts:
    - host: chart-example.local
      paths:
      - path: /
        backend:
          serviceName: chart-example.local
          servicePort: 80
  tls: []

resources:
  limits:
    memory: 2Gi
    cpu: "1"
  requests:
    memory: 100Mi
    cpu: 300m

persistence:
  type: pvc

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 1
  targetCPUUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}



# To enable the required servers. Note: episilia-log-indexer, episilia-search should be enabled for Episilia to work.

redpanda:
  enabled: &redpanda false

episilia-log-indexer:
  enabled: true

episilia-log-indexer-s3:
  enabled: false

episilia-log-indexer-opt:
  enabled: true

episilia-search:                            
  enabled: true

episilia-search-fixed:                                             # If enabled, additional topics to be created (refer fixedSearch config)
  enabled: false

episilia-search-ondemand:                                          # If enabled, additional topics to be created (refer ondemandsearch config)
  enabled: false

episilia-gateway:
  enabled: true  

episilia-spike-ui:
  enabled: true


# Each consumer will have unique client name and env provided, the same is to be used here.
global:
  client:
    id: episilia-helm   
    env: test-helm
    license:
      key: episilia  
    arn: ""                                                                        # arn

  kafka:
    group:
      search: episilia-search-group                                                # Kafka consumer-group for search
      spike: episilia-spike-group                                                  # Kafka consumer-group for spike
      cpanel: episilia-cpanel-group                                                # Kafka consumer-group for cpanel
      gateway:
        tail: episilia-gw-tail-group                                               # Kafka consumer-group for gateway tail 
      logwatcher:
        alert: episilia-lw-alert-group                                             # Kafka consumer-group for logwatcher alert 
        tail: episilia-lw-tail-group                                               # Kafka consumer-group for logwatcher tail 
      s3log:
        files: episilia-s3files-group                                              # Kafka consumer-group for s3logs
      
    topic:
      index:
        live: stagefiles                                                          # Topic for internal publish indexed files - stage.topic
        optimized: optfiles                                                       # Topic for internal optimize.topic:  publish file names post optimization
        labels: indexlabels                                                       # Topic to publish labels from indexer
      optimize:
        request: stagefolder                                                      # optimize.request.topic send folders to optimize
      s3log:
        files: s3logs                                                             # Topic from where s3 logs are loaded.
      cpanel:
        out: cpanel-out                                                              # Internal topic cpanel.data.topic
      
      tail:
        request:
          in: tail_in                                                             # incoming requests for tail requests
        response:
          out: tail_out                                                            # publish results for tail
      alert:
        response:
          out: alert_out                                                            # publish alerts for spike
    indexer:
      broker:
        list: redpanda:9092                                                          # The kafka broker for logs. If this is not set, it will use the default broker
        security:                           
          mode: none                                                                 # values are [none|login|oauth|kerberos]
          protocol: SASL_SSL                                                         # plaintext, ssl, sasl_plaintext, sasl_ssl
        sasl:
          mechanism: SCRAM-SHA-512                                                   #PLAIN, SCRAM-SHA-256, SCRAM-SHA-512, OAUTHBEARER, GSSAPI
          username: episilia
          password: episilia123
        rack:
          aware: "false"                                                              # for rack awareness make it true

      logs:
        topics: logs                                                                  # Topic from where logs are loaded.
      group: episilia-indexer-group                                                   # Kafka consumer-group for indexer
      
    internal:
      broker:
        list: redpanda:9092                                                            # The kafka broker for internal communication
  
  spike:
    JVM_MAX_SIZE: "1536m"
    JVM_NEWGEN_MIN_SIZE: "800m"
    JVM_NEWGEN_MAX_SIZE: "900m"
    metadata:
      backfill:
        forceupdate: "false"
        days: "0"  
    s3logs:
      publish:
        seconds: "2"
        partitionwise: "false"
    alert:
      publish:
        enable: "false"    
    login:
      mode: local                                                                         # local, google, okta
      local:
        password:
          encryptionkey: "i am groot"
      google:
        clientid: "google-client"
        token: "google-token"
      okta:
        clientid: "okta-client"
        token: "okta-token"
    pulse:
      access:
        key: "token"
        token: "random"
      host: "localhost"
      url: "http://localhost:8080/"

  ops:
    log:
      metrics:
        publish:
          interval:
            seconds: "300"
      state:
        publish:
          interval:
            seconds: "10"

#You can include these configurations in the relevant sections of your values.yaml file to match your specific needs. Make sure to maintain the indentation and structure according to your Helm chart requirements.

  datastore:
    stage:
      s3:
        accesskey: ""                                                                          # filestorage access key (eg: AWS S3 access key)
        secretkey: ""                                                                          # filestorage secret key (eg: AWS S3 secret key)   
        region: ""                                                                             # filestorage region (eg: AWS S3 region)
        endpoint:
          url: ""                                                                                 # filestorage endpoint URL (eg: AWS S3 endpoint URL)
        sign:
          payload: true
        bucket: episilia-bucket                                                                # filestorage bucket (eg: AWS S3 bucket)
        folder: episilia-folder                                                                # filestorage folder URL (eg: AWS S3 folder)
        work:
          folder: work-folder
        url:
          prefix: s3://
        useArn: false
        assumeRole: ""                                                                          # ARN role
        roleArn: ""
        pathstyle:
          access: "false"
        https: "true"
    
    final:
      s3:
        accesskey: ""                                                                            # filestorage access key (eg: AWS S3 access key)
        secretkey: ""                                                                            # filestorage secret key (eg: AWS S3 secret key)   
        region: ""                                                                               # filestorage region (eg: AWS S3 region)
        endpoint:
          url: ""                                                                                   # filestorage endpoint URL (eg: AWS S3 endpoint URL)
        sign:
          payload: true
        bucket: episilia-bucket                                                                   # filestorage bucket (eg: AWS S3 bucket)
        folder: episilia-folder                                                                   # filestorage folder URL (eg: AWS S3 folder)
        work:
          folder: work-folder
        url:
          prefix: s3://
        useArn: false
        assumeRole: ""                                                                             # ARN role
        roleArn: "arn:aws:iam::<redacted>:role/role-for-helm-chart"
        pathstyle:
          access: "false"
        https: "true"

    sourcebucket:
      enabled: true                                                                                # to load logs from s3 bucket
      s3:
        accesskey: ""                                                                               # filestorage access key (eg: AWS S3 access key)
        secretkey: ""                                                                               # filestorage secret key (eg: AWS S3 secret key)   
        region: ""                                                                                  # filestorage region (eg: AWS S3 region)
        endpoint:
          url: ""                                                                                    # filestorage endpoint URL (eg: AWS S3 endpoint URL)
        sign:
          payload: true
        bucket: episilia-bucket                                                                     # filestorage bucket (eg: AWS S3 bucket)
        folder: episilia-folder                                                                     # filestorage folder URL (eg: AWS S3 folder)
        work:
          folder: work-folder
        url:
          prefix: s3://
        useArn: false
        assumeRole: ""                                                                              # ARN role
        roleArn: ""                                      
        sourcefolder: sourcefolder
        pathstyle:
          access: "true"
        https: "false"

# Config for indexer and optimizer

  indexer:
    image:
      repository: episilia/log-indexer                                                                # docker image of episilia-log-indexer
      tag: *release
    replicaCount: "1"                                                                                 # kubernetes pod replicas of episilia-log-indexer
    annotations:
      deploy:
      service:
    resources:
      limits:
        cpu: "1"                                                                                      # cpu limit on episilia-log-indexer 
        memory: 2Gi                                                                                   # memory limit on episilia-log-indexer
      requests:
        cpu: 400m                                                                                     # cpu request on episilia-log-indexer
        memory: 300Mi                                                                                 # memory request on episilia-log-indexer

    schema:
      appid:
        fixed: "defaultApp"                                                                          # If appid is a fixed string
        keys: "app_id"                                                                              # label(s)     for app identifier, ex: "kubernetes_container_name"
      tenantid:        
        fixed: "defaultTenant"                                                                      # If tenantid is a fixed string  
        keys: "tenant_id"                                                                           # label(s) for tenant identifier, ex: "kubernetes_namespace_name"        
      message:
        key: "log"                                                                                  # actual log message key
      timestamp:
        key: "time"                                                                                 # timestamp key
        formats: "%Y-%m-%dT%H:%M:%S"                                                                # to specify timestamp format (ex: %Y-%m-%dT%H:%M:%S )
      exclude: "timestamp"                                                                          # labels to be excluded from the list

    logs:
      source: kafka                                                                                 # source: kafka  # s3 or kafka
    tail:
      enable: "true"                                                                                # to enable tail server
      maxwait:
        ms: "5000"                                                                                  # time to get tail logs
    alert:
      enable: "false"                                                                                # to enable alert server
      rules:
        file:
          url: "s3://bucket/folder/file"                                                           # alert config url need to be update here
      prometheus:
        gateway: localhost:5070                                                                    # push-gateway url 

    ops:
      pause: 
        consume:                                                                                   #pauses injest at the below thresholds
          file:
           max:
             count: "100"                                                                          # applicable for file messages
          record:
            max:
              count: "500000"                                                                      # applicable for log messages
            size:
              max: 
                mb: "100"                                                                          # applicable for log messages
      datablock:
        writer:
          count: "1"                                                                               # datablocks zipped and written to disk, in sequence files
      json:
        processor:
          count: "1"                                                                               # number of json parsers
    optimize:
      block:
        maxbytes:
          mb: "10"


  # Indexing s3 logs
  indexers3:
    image:
      repository: episilia/log-indexer                                                                                     # docker image of episilia-log-indexer
      tag: *release
    replicaCount: "1"                                                                                                      # kubernetes pod replicas of episilia-log-indexer-s3
    resources:
      limits:
        cpu: "1"                                                                                                           # cpu limit on episilia-log-indexer-s3
        memory: 2Gi                                                                                                        # memory limit on episilia-log-indexer-s3
      requests:
        cpu: 400m                                                                                                          # cpu request on episilia-log-indexer-s3
        memory: 300Mi                                                                                                      # memory request on episilia-log-indexer-s3


  #optimizer server configuration    
  optimizer:
    replicaCount: "1"                                                                                                       # kubernetes pod replicas of episilia-optimizer  
    resources:
      limits:
        cpu: "1"                                                                                                            # cpu limit on episilia-optimizer
        memory: 2Gi                                                                                                         # memory limit on episilia-optimizer
      requests:
        cpu: 500m                                                                                                           # cpu request on episilia-optimizer        
        memory: 300Mi 
   
  
  #config for search engine
  livesearch:
    image:
      repository: episilia/search                                                                                           # docker image of episilia-search
      tag: *release
    
    replicaCount: "1"                                                                                                      # kubernetes pod replicas of episilia-search 
    resources:
      limits:
        cpu: "1"                                                                                                           # cpu limit on episilia-search
        memory: 2Gi                                                                                                        # memory limit on episilia-search
      requests:
        cpu: 500m                                                                                                          # cpu request on episilia-search
        memory: 600Mi                                                                                                      # memory request on episilia-search
    
    api:
      timeout:
        seconds: 60                                                                                                         # timeout for search while querying
      request:
        max:
          days: "30"                                                                                                        # days for querying search
    
    live:
      from:
        hours: 48                                                                                                           # hours from when the required index blocks should be loaded
      to:
        hours: 0                                                                                                           # hours till when the required index blocks should be loaded, Note: value to be "0" to get instant logs

    ops:
      index:
        cache:
          resetonstart: "true"
    
    labels:
      display:
        max:
          count: "1000"                                                                                                     # labels count displayed in spikeui/grafana
      exclude: "@timestamp,log"                                                                                             # Lables excluded from spikeui/grafana dropdown GUI. 
    

    
  #config for search-ondemand engine
  ondemandsearch:
    replicaCount: "1"                                                                                                     # kubernetes pod replicas of episilia-search-ondemand
    resources:
      limits:
        cpu: "1"                                                                                                          # cpu limit on episilia-search-ondemand
        memory: 2Gi                                                                                                       # memory limit on episilia-search-ondemand
      requests:
        cpu: 500m                                                                                                         # cpu request on episilia-search-ondemand
        memory: 600Mi                                                                                                     # memory request on episilia-search-ondemand                                
    prewarm:
      enabled: "false"
      # set either yyyymmddhh pair or hours pair. If both are set the hours pair will be considered
      from:
        hours: "2"                                                                                                         # hours from when the required labels should be loaded
        yyyymmddhh: "2023102100"                                                                                           # the date from when the required labels should be loaded (YYYYMMDDHH) or mske it "0" to load from hours
      to:
        hours: "0"                                                                                                         # hours till when the required labels should be loaded
        yyyymmddhh: "2023102100"                                                                                           # the date till when the required labels should be loaded (YYYYMMDDHH) or mske it "0" to load from hours
    ops:
      index:
        cache:
          s3list:
            seconds: "600"                           
    topic:
      index:
        live: "stagefile-ondemand"                                                                                         # Topic for internal publish indexed files for ondemand
        optimized: "optfile-ondemand"                                                                                      # Topic to publish file names post optimization for ondemand


  #config for historic search engine
  fixedSearch:
    bucket: ""                                                                                                              # s3 bucket for historic search to run parallelly, Note: if the value is empty it takes datastore.s3.bucket value as default
    folder: ""                                                                                                              # s3 folder for historic search to run parallelly, Note: if the value is empty it takes datastore.s3.folder value as default
    replicaCount: "1"                                                                                                      # kubernetes pod replicas of historic episilia-search 
    resources:
      limits:
        cpu: "1"                                                                                                             # cpu limit on historic episilia-search
        memory: 2Gi                                                                                                          # memory limit on historic episilia-search
      requests:
        cpu: 500m                                                                                                            # cpu request on historic episilia-search
        memory: 600Mi                                                                                                        # memory request on historic episilia-search
    fixed:
      from:
        yyyymmddhh: "2023102100"                                                                                              # the date from when the required index blocks should be loaded (YYYYMMDDHH)
      to:
        yyyymmddhh: "2023102202"                             # the date till when the required index blocks should be loaded (YYYYMMDDHH)

    api:
      timeout:
        seconds: 60                                          # timeout for search while querying
    topic:
      index:
        live: "stagefile-fixed"                              # Topic for internal publish indexed files for fixed search
        optimized: "optfile-fixed"                           # Topic to publish file names post optimization for fixed search


# tail server configurations
  gateway:
    replicaCount: "1"                                   # kubernetes pod replicas of episilia-gateway
    image:
      repository: episilia/gateway                      # docker image of episilia-gateway
      tag: *release
    resources:
      limits:
        cpu: 500m                                       # cpu limit on episilia-gateway
        memory: 600Mi                                   # memory limit on episilia-gateway
      requests:
        cpu: 300m                                       # cpu request on episilia-gateway        
        memory: 300Mi                                    # memory request on episilia-gateway    
    service: 
      type: "ClusterIP"  
    annotations:
      service.beta.kubernetes.io/aws-load-balancer-internal: "false"


# spike ui server configurations
  spikeui:
    replicaCount: "1"                               # kubernetes pod replicas of episilia-spike-ui
    image:
      repository: episilia/spike-ui                 # docker image of episilia-spike-ui
      tag: *release
    resources:
      limits:
        cpu: 500m                                  # cpu limit on episilia-spike -ui
        memory: 800Mi                              # memory limit on episilia-spike -ui
      requests:
        cpu: 300m                                  # cpu request on episilia-spike -ui        
        memory: 500Mi                              # memory request on episilia-spike-ui
    service: 
      type: "LoadBalancer"  
    annotations:
      service.beta.kubernetes.io/aws-load-balancer-internal: "false"

 
# Redpanda
  redpanda:                                       # kubernetes pod replicas of episilia-gateway
    enabled: *redpanda
    resources:
      limits:
        cpu: 1000m                                # cpu limit on redpanda
        memory: 2048Mi                           # memory limit on redpanda
      requests:
        cpu: 300m                                # cpu request on redpanda
        memory: 600Mi                            # memory request on redpanda
    ports:
      admin: "9644"
      kafka: "9092"
      proxy: "8082"
    pvcSize: "10Gi"
    provMem: "1G"
    resMem: "200M"
    retention: "10800000"

        
 # Persistance Volume   
  persistence:  
    enabled: "true"
    storageClassName: gp2                       # storage class name (differs on the cloud services that are used)
    accessModes:
    - ReadWriteOnce                             # access modes
    size: "10Gi"                                # size of PVC which will be mounted to episilia-search for live search
    historicSize: "5Gi"                         # size of PVC which will be mounted to episilia-search for historic search
    ondemandSize: "10Gi"                        # size of PVC which will be mounted to episilia-search for ondemand search
    spikeSize: "10Gi"                           # size of PVC which will be mounted to episilia-spike for spike 
    # annotations: {}
    finalizers:
     - kubernetes.io/pvc-protection
    # selectorLabels: {}
    # subPath: ""
    # existingClaim:


  
